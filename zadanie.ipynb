{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T20:05:04.200368Z",
     "start_time": "2025-10-14T20:05:04.181734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Removed gensim import - implementing Word2Vec from scratch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Note: tf_keras imports removed - using custom tokenization instead\n",
    "try:\n",
    "    import kagglehub\n",
    "    KAGGLE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    KAGGLE_AVAILABLE = False\n",
    "    print(\"Warning: kagglehub not available, will use dummy data for demonstration\")\n",
    "\n",
    "# Ukrywamy denerwujące komunikaty\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# --- USTAWIANIE ZMIENNYCH ---\n",
    "MAX_SEQUENCE_LENGTH = 300   # Max długość recenzji\n",
    "EMBEDDING_DIM = 100         # Wymiar Word2Vec. Ma być 100\n",
    "W2V_MIN_COUNT = 5           # Olewamy słowa, które są rzadkie\n",
    "\n",
    "LSTM_HIDDEN_DIM = 128\n",
    "NUM_CLASSES = 2             # Klasy: pozytywna (1) lub negatywna (0)\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "BIDIRECTIONAL = True        # Używamy LSTM dwukierunkowego\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "\n",
    "# --- KLASY MODELU PYTORCH ---\n",
    "\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    # Model LSTM z polecenia\n",
    "    def __init__(self, embedding_dim, lstm_hidden_dim, num_layers, num_classes, dropout=0.3, bidirectional=False):\n",
    "        super(EmbeddingClassifier, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=lstm_hidden_dim, num_layers=num_layers,\n",
    "                             batch_first=True, dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        direction_factor = 2 if bidirectional else 1\n",
    "\n",
    "        # Warstwa do podjęcia ostatecznej decyzji\n",
    "        self.fc = nn.Linear(lstm_hidden_dim * direction_factor, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "\n",
    "        # Bierzemy ostatni stan (Hidden State)\n",
    "        if self.bidirectional:\n",
    "            # Łączymy stany z przodu i z tyłu\n",
    "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1, :, :]\n",
    "\n",
    "        out = self.fc(hidden)\n",
    "        return out\n",
    "\n",
    "class TextEmbeddingDataset(Dataset):\n",
    "    # Klasa, która zamienia ID na wektory Word2Vec\n",
    "    def __init__(self, sequences_padded, labels, embedding_matrix):\n",
    "        self.sequences = sequences_padded\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.embedding_matrix = torch.from_numpy(embedding_matrix)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.sequences[idx]\n",
    "\n",
    "        # Zamieniamy ID słów na wektory Word2Vec\n",
    "        embeddings = torch.index_select(self.embedding_matrix, 0, torch.LongTensor(indices))\n",
    "\n",
    "        return embeddings, self.labels[idx]\n",
    "\n",
    "\n",
    "# --- KROK 1: POBIERANIE DANYCH ---\n",
    "\n",
    "print(\"Pobieramy IMDb z Kaggle Hub...\")\n",
    "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "\n",
    "csv_file_path = os.path.join(path, \"IMDB Dataset.csv\")\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    print(\"Wczytane.\")\n",
    "else:\n",
    "    # Dummy data for demonstration if kagglehub not available\n",
    "    print(\"Using dummy data (kagglehub not available)\")\n",
    "    df = pd.DataFrame({\n",
    "        'review': ['great movie', 'terrible film'] * 100,\n",
    "        'sentiment': ['positive', 'negative'] * 100\n",
    "    })\n",
    "\n",
    "all_reviews_text = df['review'].tolist()\n",
    "# Zamieniamy \"positive\"/\"negative\" na 1/0\n",
    "all_labels = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n",
    "\n",
    "# Dzielimy dane na treningowe i testowe\n",
    "X_train_text, X_test_text, Y_train, Y_test = train_test_split(\n",
    "    all_reviews_text, all_labels, test_size=TEST_SIZE, random_state=42, stratify=all_labels\n",
    ")\n",
    "\n",
    "\n",
    "# --- KROK 2: NORMALIZACJA I TRENOWANIE WORD2VEC (PRACA DOMOWA) ---\n",
    "\n",
    "# Normalizacja: usuwamy HTML i znaki, wszystko małe litery\n",
    "tokenized_corpus = [\n",
    "    re.sub(r'[^a-z\\s]', '', re.sub(r'<br />', ' ', review).lower()).split()\n",
    "    for review in all_reviews_text\n",
    "]\n",
    "\n",
    "print(\"Zaczynamy Word2Vec...\")\n",
    "start_w2v_time = time.time()\n",
    "\n",
    "# Trenowanie Word2Vec (custom implementation)\n",
    "w2v_model = SimpleWord2Vec(\n",
    "    vector_size=EMBEDDING_DIM,\n",
    "    window=5,\n",
    "    min_count=W2V_MIN_COUNT,\n",
    "    sg=1  # Skip-gram\n",
    ")\n",
    "w2v_model.fit(tokenized_corpus, vector_size=EMBEDDING_DIM, window=5, min_count=W2V_MIN_COUNT, workers=4, sg=1)\n",
    "\n",
    "end_w2v_time = time.time()\n",
    "print(f\"Word2Vec skończony. Wymiar: {EMBEDDING_DIM}, Czas: {end_w2v_time - start_w2v_time:.2f}s\")\n",
    "\n",
    "\n",
    "# --- KROK 3: PRZYGOTOWANIE DO PYTORCH ---\n",
    "\n",
    "# Tworzymy słownik ID słów\n",
    "keras_tokenizer = Tokenizer()\n",
    "keras_tokenizer.fit_on_texts(all_reviews_text)\n",
    "\n",
    "word_index = keras_tokenizer.word_index\n",
    "VOCAB_SIZE = len(word_index) + 1\n",
    "\n",
    "# Tworzymy Macierz Embeddingów\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM), dtype=np.float32)\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word] # Kopiujemy wektor Word2Vec\n",
    "\n",
    "# Zamieniamy słowa na ich ID (sekwencjonowanie)\n",
    "X_train_sequences = keras_tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test_sequences = keras_tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "# Wyrównanie długości (Padding)\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# Czy jest GPU\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "# Tworzymy DataLoadery dla PyTorcha\n",
    "train_dataset = TextEmbeddingDataset(X_train_padded, Y_train, embedding_matrix)\n",
    "test_dataset = TextEmbeddingDataset(X_test_padded, Y_test, embedding_matrix)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=pin_memory)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "\n",
    "# --- KROK 4: FUNKCJE TRENINGU ---\n",
    "\n",
    "def train_model(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache() # Dla GPU\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # Zerujemy gradienty\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward() # Wsteczna propagacja\n",
    "        optimizer.step() # Aktualizacja wag\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad(): # Wyłączamy liczenie gradientów\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# --- KROK 5: TRENING W PĘTLI ---\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Używamy: {device}\")\n",
    "\n",
    "# Tworzymy model LSTM\n",
    "model = EmbeddingClassifier(\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    lstm_hidden_dim=LSTM_HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dropout=DROPOUT,\n",
    "    bidirectional=BIDIRECTIONAL\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"\\nStart treningu PyTorch LSTM...\")\n",
    "start_lstm_time = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "\n",
    "    print(f\"Epoka {epoch}/{EPOCHS} (Czas: {epoch_duration:.2f} s)\")\n",
    "    print(f\"  Trening: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
    "    print(f\"  Test:    Loss={test_loss:.4f}, Acc={test_acc:.2f}%\")\n",
    "\n",
    "end_lstm_time = time.time()\n",
    "\n",
    "print(\"Koniec treningu.\")\n",
    "\n",
    "# --- KROK 6: ZAPISANIE WAG ---\n",
    "\n",
    "SAVE_PATH = 'imdb_sentiment_model_weights.pth'\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "print(f\"\\nWagi zapisane: {SAVE_PATH}\")\n",
    "print(f\"\\nCałkowity czas Word2Vec: {end_w2v_time - start_w2v_time:.2f}s\")\n",
    "print(f\"Całkowity czas treningu LSTM: {end_lstm_time - start_lstm_time:.2f}s\")"
   ],
   "id": "81f54aebe3162877",
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2479242846.py, line 96)",
     "output_type": "error",
     "traceback": [
      "  \u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 96\u001B[39m\n\u001B[31m    \u001B[39m\u001B[31mdf = pd.read_csv(csv_file_path)\u001B[39m\n    ^\n\u001B[31mIndentationError\u001B[39m\u001B[31m:\u001B[39m unexpected indent\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "29537e4f3e4082ad",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
