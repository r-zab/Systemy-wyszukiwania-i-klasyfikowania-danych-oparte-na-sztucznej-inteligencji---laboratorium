{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T12:11:32.575129Z",
     "start_time": "2025-10-05T12:11:15.598629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "train_data = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "test_data  = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "X_train, y_train = train_data.data, train_data.target\n",
    "X_test, y_test = test_data.data, test_data.target\n",
    "\n",
    "print(f\"Liczba dokumentów w zbiorze treningowym: {len(X_train)}\")\n",
    "print(f\"Liczba dokumentów w zbiorze testowym: {len(X_test)}\")\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "X_train_tokens = [clean_and_tokenize(doc) for doc in X_train]\n",
    "X_test_tokens = [clean_and_tokenize(doc) for doc in X_test]\n",
    "\n",
    "print(\"\\nPrzykład tokenów z 1. dokumentu (pierwsze 30 tokenów):\")\n",
    "print(X_train_tokens[0][:30])"
   ],
   "id": "2e80f537f1bce972",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rafal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba dokumentów w zbiorze treningowym: 11314\n",
      "Liczba dokumentów w zbiorze testowym: 7532\n",
      "\n",
      "Przykład tokenów z 1. dokumentu (pierwsze 30 tokenów):\n",
      "['i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', 'the', 'other', 'day', 'it', 'was', 'a', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late']\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Zadanie domowe\n",
   "id": "b1813af4238b5d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T12:12:40.980382Z",
     "start_time": "2025-10-05T12:11:32.610376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow_datasets as tfds # Do ładowania zbioru IMDB\n",
    "import numpy as np                 # Do ogólnych operacji numerycznych\n",
    "import re                          # Do wyrażeń regularnych (czyszczenia tekstu)\n",
    "import nltk                        # Do operacji NLP\n",
    "from nltk.tokenize import word_tokenize # Do dzielenia tekstu na słowa (tokeny)\n",
    "\n",
    "# Pobieranie niezbędnego pakietu NLTK do tokenizacji.\n",
    "# Wykonuje się tylko raz.\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# --- KROK 1: ŁADOWANIE DANYCH IMDB REVIEWS ---\n",
    "\n",
    "# Ładowanie zbioru danych IMDB Reviews\n",
    "# split=['train', 'test']: dzieli na zbiór treningowy i testowy\n",
    "# as_supervised=True: zwraca dane jako krotki (tekst, etykieta)\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'imdb_reviews',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "# Konwersja zbiorów TensorFlow na listy Pythona\n",
    "# Użycie tfds.as_numpy() konwertuje dane, a następnie:\n",
    "# - doc (recenzja) to ciąg bajtów (bytes), który musimy zdekodować do str\n",
    "# - label (sentyment: 0 lub 1) jest już gotowy\n",
    "print(\"--- Ładowanie i konwersja danych ---\")\n",
    "\n",
    "# Zbiór treningowy (tekst i etykiety)\n",
    "X_train_imdb = [doc.decode('utf8') for doc, label in tfds.as_numpy(ds_train)]\n",
    "y_train_imdb = [label for doc, label in tfds.as_numpy(ds_train)]\n",
    "\n",
    "# Zbiór testowy (tekst i etykiety)\n",
    "X_test_imdb = [doc.decode('utf8') for doc, label in tfds.as_numpy(ds_test)]\n",
    "y_test_imdb = [label for doc, label in tfds.as_numpy(ds_test)]\n",
    "\n",
    "print(f\"Liczba recenzji treningowych: {len(X_train_imdb)}\") # Powinno być 25000\n",
    "print(f\"Liczba recenzji testowych: {len(X_test_imdb)}\")     # Powinno być 25000\n",
    "\n",
    "\n",
    "# --- KROK 2: DEFINICJA FUNKCJI CZYSZCZĄCEJ I TOKENIZUJĄCEJ ---\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    # 1. Usuń znaki niebędące literami (zachowaj a-z, A-Z i białe znaki)\n",
    "    # Odpowiada to funkcji clean_text ze zrzutu\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "\n",
    "    # 2. Zmień wszystkie litery na małe\n",
    "    text = text.lower()\n",
    "\n",
    "    # 3. Usuń nadmiarowe białe znaki (np. podwójne spacje po usunięciu znaków) i przytnij\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # 4. Tokenizacja (podział na pojedyncze słowa/tokeny)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# --- KROK 3: ZASTOSOWANIE CZYSZCZENIA I TOKENIZACJI ---\n",
    "\n",
    "print(\"\\n--- Rozpoczęcie czyszczenia i tokenizacji ---\")\n",
    "\n",
    "# Zastosowanie funkcji do danych treningowych\n",
    "X_train_tokens_imdb = [clean_and_tokenize(doc) for doc in X_train_imdb]\n",
    "\n",
    "# Zastosowanie funkcji do danych testowych\n",
    "X_test_tokens_imdb = [clean_and_tokenize(doc) for doc in X_test_imdb]\n",
    "\n",
    "\n",
    "# --- KROK 4: WERYFIKACJA WYNIKÓW ---\n",
    "\n",
    "print(\"\\n--- Przykład przetworzonego dokumentu ---\")\n",
    "print(f\"Oryginalna recenzja (fragment): {X_train_imdb[0][:150]}...\")\n",
    "print(f\"Sentyment (0=negatywny, 1=pozytywny): {y_train_imdb[0]}\")\n",
    "print(\"Tokeny po przetworzeniu (pierwsze 30):\")\n",
    "print(X_train_tokens_imdb[0][:30])\n",
    "\n",
    "# Teraz zmienne X_train_tokens_imdb i X_test_tokens_imdb zawierają listy list słów (tokenów),\n",
    "# gotowe do dalszej analizy (np. liczenia częstości słów lub budowy modelu)."
   ],
   "id": "3a1aa92af40fecbe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rafal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ładowanie i konwersja danych ---\n",
      "Liczba recenzji treningowych: 25000\n",
      "Liczba recenzji testowych: 25000\n",
      "\n",
      "--- Rozpoczęcie czyszczenia i tokenizacji ---\n",
      "\n",
      "--- Przykład przetworzonego dokumentu ---\n",
      "Oryginalna recenzja (fragment): This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be the...\n",
      "Sentyment (0=negatywny, 1=pozytywny): 0\n",
      "Tokeny po przetworzeniu (pierwsze 30):\n",
      "['this', 'was', 'an', 'absolutely', 'terrible', 'movie', 'don', 't', 'be', 'lured', 'in', 'by', 'christopher', 'walken', 'or', 'michael', 'ironside', 'both', 'are', 'great', 'actors', 'but', 'this', 'must', 'simply', 'be', 'their', 'worst', 'role', 'in']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T12:13:49.488730Z",
     "start_time": "2025-10-05T12:12:41.650360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- IMPORTY I KONFIGURACJA ---\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Pobranie pakietu NLTK 'punkt' do tokenizacji\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# --- ŁADOWANIE DANYCH IMDB REVIEWS ---\n",
    "\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'imdb_reviews',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "# Konwersja zbiorów TensorFlow na listy Pythona (poprawka błędu z .numpy())\n",
    "X_train_imdb = [doc.decode('utf8') for doc, label in tfds.as_numpy(ds_train)]\n",
    "y_train_imdb = [label for doc, label in tfds.as_numpy(ds_train)]\n",
    "\n",
    "X_test_imdb = [doc.decode('utf8') for doc, label in tfds.as_numpy(ds_test)]\n",
    "y_test_imdb = [label for doc, label in tfds.as_numpy(ds_test)]\n",
    "\n",
    "print(f\"Dane treningowe: {len(X_train_imdb)} recenzji\")\n",
    "print(f\"Dane testowe: {len(X_test_imdb)} recenzji\")\n",
    "\n",
    "\n",
    "# --- FUNKCJA CZYSZCZĄCA I TOKENIZUJĄCA ---\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    # Usunięcie znaków specjalnych i cyfr\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    # Zmiana na małe litery\n",
    "    text = text.lower()\n",
    "    # Usunięcie nadmiarowych białych znaków\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Tokenizacja\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# --- PRZETWARZANIE DANYCH ---\n",
    "\n",
    "X_train_tokens_imdb = [clean_and_tokenize(doc) for doc in X_train_imdb]\n",
    "X_test_tokens_imdb = [clean_and_tokenize(doc) for doc in X_test_imdb]\n",
    "\n",
    "\n",
    "# --- WERYFIKACJA ---\n",
    "print(\"\\nPrzykład tokenów (pierwsze 30):\")\n",
    "print(X_train_tokens_imdb[0][:30])"
   ],
   "id": "2330fd2a341622f9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rafal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dane treningowe: 25000 recenzji\n",
      "Dane testowe: 25000 recenzji\n",
      "\n",
      "Przykład tokenów (pierwsze 30):\n",
      "['this', 'was', 'an', 'absolutely', 'terrible', 'movie', 'don', 't', 'be', 'lured', 'in', 'by', 'christopher', 'walken', 'or', 'michael', 'ironside', 'both', 'are', 'great', 'actors', 'but', 'this', 'must', 'simply', 'be', 'their', 'worst', 'role', 'in']\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
