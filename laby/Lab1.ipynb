{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "train_data = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "test_data  = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "X_train, y_train = train_data.data, train_data.target\n",
    "X_test, y_test = test_data.data, test_data.target\n",
    "print(f\"Liczba dokumentów w zbiorze treningowym: {len(X_train)}\")\n",
    "print(f\"Liczba dokumentów w zbiorze testowym: {len(X_test)}\")\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "X_train_tokens = [clean_and_tokenize(doc) for doc in X_train]\n",
    "X_test_tokens = [clean_and_tokenize(doc) for doc in X_test]\n",
    "print(\"\\nPrzykład tokenów z 1. dokumentu (pierwsze 30 tokenów):\")\n",
    "print(X_train_tokens[0][:30])"
   ],
   "id": "2e80f537f1bce972",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Zadanie domowe\n",
   "id": "b1813af4238b5d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# IMPORTY I KONFIGURACJA\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Pobranie pakietu NLTK 'punkt' do tokenizacji\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ŁADOWANIE DANYCH IMDB REVIEWS\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'imdb_reviews',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "# Konwersja zbiorów TensorFlow na listy Pythona\n",
    "X_train_imdb = [doc.decode('utf8') for doc, label in tfds.as_numpy(ds_train)]\n",
    "y_train_imdb = [label for doc, label in tfds.as_numpy(ds_train)]\n",
    "X_test_imdb = [doc.decode('utf8') for doc, label in tfds.as_numpy(ds_test)]\n",
    "y_test_imdb = [label for doc, label in tfds.as_numpy(ds_test)]\n",
    "print(f\"Dane treningowe: {len(X_train_imdb)} recenzji\")\n",
    "print(f\"Dane testowe: {len(X_test_imdb)} recenzji\")\n",
    "\n",
    "\n",
    "# FUNKCJA CZYSZCZĄCA I TOKENIZUJĄCA\n",
    "def clean_and_tokenize(text):\n",
    "    # Usunięcie znaków specjalnych i cyfr\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    # Zmiana na małe litery\n",
    "    text = text.lower()\n",
    "    # Usunięcie nadmiarowych białych znaków\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Tokenizacja\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# PRZETWARZANIE DANYCH\n",
    "X_train_tokens_imdb = [clean_and_tokenize(doc) for doc in X_train_imdb]\n",
    "X_test_tokens_imdb = [clean_and_tokenize(doc) for doc in X_test_imdb]\n",
    "\n",
    "# WERYFIKACJA\n",
    "print(\"\\nPrzykład tokenów (pierwsze 30):\")\n",
    "print(X_train_tokens_imdb[0][:30])"
   ],
   "id": "2330fd2a341622f9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
