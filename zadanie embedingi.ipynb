{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Zadanie tokenizacja",
   "id": "bfc4224c527d0800"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- IMPORTOWANIE NARZĘDZI ---\n",
    "import tensorflow_datasets as tfds  # do ściągania gotowych zbiorów danych, w tym imdb\n",
    "import numpy as np                 # do podstawowych operacji na danych, np. tablicach\n",
    "import re                          # do wyrażeń regularnych, czyli czyszczenia tekstu\n",
    "import nltk                        # Natural Language Toolkit (nasza baza NLP)\n",
    "from nltk.tokenize import word_tokenize  # konkretna funkcja do dzielenia tekstu na słowa\n",
    "\n",
    "try:\n",
    "    # ściągamy 'punkt', to są zasoby potrzebne do poprawnej tokenizacji przez NLTK\n",
    "    nltk.download('punkt')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# --- ŁADOWANIE ZBIORU DANYCH IMDB ---\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'imdb_reviews',                   # jaki zbiór danych chcemy\n",
    "    split=['train', 'test'],          # od razu dzielimy na trening i test\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,               # dane w formacie (tekst, etykieta)\n",
    "    with_info=True)\n",
    "\n",
    "# przerabiamy dane z tfds na zwykłe listy stringów i listę etykiet (0/1)\n",
    "# dekodujemy tekst z binarnego formatu na czytelny utf8\n",
    "X_train_imdb = [doc.decode('utf8') for doc, label in tfds.as_numpy(ds_train)]\n",
    "y_train_imdb = [label for doc, label in tfds.as_numpy(ds_train)]\n",
    "X_test_imdb = [doc.decode('utf8') for doc, label in tfds.as_numpy(ds_test)]\n",
    "y_test_imdb = [label for doc, label in tfds.as_numpy(ds_test)]\n",
    "\n",
    "print(f\"Dane treningowe: {len(X_train_imdb)} recenzji\")\n",
    "print(f\"Dane testowe: {len(X_test_imdb)} recenzji\")\n",
    "\n",
    "# --- KLUCZOWA FUNKCJA: CZYSZCZENIE I TOKENIZACJA ---\n",
    "def clean_and_tokenize(text):\n",
    "    # 1. CZYSZCZENIE: usuwamy wszystko co nie jest literą (a-z, A-Z) ani spacją, zastępując spacją.\n",
    "    # pozbywamy się interpunkcji i znaków specjalnych\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "\n",
    "    # 2. NORMALIZACJA: wszystko na małe litery, żeby 'Film' i 'film' były takie same\n",
    "    text = text.lower()\n",
    "\n",
    "    # 3. CZYSZCZENIE SPACJI: usuwamy wielokrotne spacje i spacje z początku/końca\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # 4. TOKENIZACJA: dzielimy tekst na listę pojedynczych słów (tokenów)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# --- APLIKACJA FUNKCJI NA DANYCH ---\n",
    "# puszczamy naszą funkcję przez wszystkie recenzje treningowe\n",
    "X_train_tokens_imdb = [clean_and_tokenize(doc) for doc in X_train_imdb]\n",
    "# i przez wszystkie recenzje testowe\n",
    "X_test_tokens_imdb = [clean_and_tokenize(doc) for doc in X_test_imdb]\n",
    "\n",
    "# --- PODGLĄD WYNIKU ---\n",
    "print(\"\\nPrzykład tokenów (pierwsze 30):\")\n",
    "# sprawdzamy, jak wygląda pierwsza recenzja po czyszczeniu i tokenizacji\n",
    "print(X_train_tokens_imdb[0][:30])"
   ],
   "id": "4e65fb00b868f4cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Zadanie ocena sentymentu opini IMBD",
   "id": "9bc2a0b761639ea"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time # obliczanie czasu treningu, żeby wiedzieć ile to zajmuje\n",
    "\n",
    "# importy z TensorFlow/Keras – to tylko do tokenizera i paddingu!\n",
    "from tf_keras.preprocessing.text import Tokenizer\n",
    "from tf_keras.preprocessing.sequence import pad_sequences\n",
    "import kagglehub\n",
    "\n",
    "# ukrycie komunikatów INFO i WARNING od TensorFlow/oneDNN, żeby nie zaśmiecać konsoli\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# --- KONFIGURACJA STAŁYCH (NASZE HIPERPARAMETRY) ---\n",
    "MAX_SEQUENCE_LENGTH = 300   # max długość recenzji, do tego będziemy przycinać/wypełniać\n",
    "EMBEDDING_DIM = 100         # wymiar wektora słowa Word2Vec (każde słowo ma 100 liczb)\n",
    "W2V_MIN_COUNT = 5           # ignorujemy słowa, które występują rzadziej niż 5 razy (bo są za rzadkie)\n",
    "LSTM_HIDDEN_DIM = 128       # rozmiar warstwy ukrytej LSTM\n",
    "NUM_CLASSES = 2             # dwie klasy: pozytywny / negatywny\n",
    "NUM_LAYERS = 3              # ile warstw LSTM nałożymy na siebie\n",
    "DROPOUT = 0.3               # regularyzacja, żeby model nie przetrenował się na pamięć\n",
    "LEARNING_RATE = 0.001       # jak szybko model się uczy\n",
    "EPOCHS = 5                  # ile razy przelecimy przez cały zbiór danych\n",
    "BATCH_SIZE = 64             # ile recenzji naraz wrzucamy do modelu\n",
    "TEST_SIZE = 0.2             # 20% danych zostawiamy na test\n",
    "\n",
    "# --- DEFINICJE KLAS PYTORCH (NASZ MODEL I SZTUCZKI) ---\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    to jest warstwa Attention (uwagi), decyduje, które słowa są najważniejsze w recenzji\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        # warstwa liniowa, która ma się nauczyć, ile \"wagi\" dać każdemu słowu\n",
    "        self.attention_layer = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, lstm_outputs):\n",
    "        # lstm_outputs shape: (batch_size, seq_len, hidden_dim) - stany ukryte z każdego słowa\n",
    "\n",
    "        # obliczamy wstępną \"ważność\" każdego słowa\n",
    "        attention_weights = self.attention_layer(lstm_outputs).squeeze(2)\n",
    "\n",
    "        # ZASTOSOWANIE SOFTMAX: to klucz, żeby wagi sumowały się do 1, czyli dostajemy % ważności każdego słowa\n",
    "        soft_attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "\n",
    "        # obliczamy ważoną sumę stanów ukrytych, to jest nasz \"wektor kontekstu\"\n",
    "        # ten wektor reprezentuje całą recenzję, z naciskiem na słowa z wysoką wagą\n",
    "        context_vector = torch.bmm(soft_attention_weights.unsqueeze(1), lstm_outputs).squeeze(1)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    główna architektura: word2vec embeddings -> lstm -> attention -> klasyfikator\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, lstm_hidden_dim, num_layers, num_classes, dropout=0.3):\n",
    "        super(EmbeddingClassifier, self).__init__()\n",
    "\n",
    "        # definicja warstwy LSTM. przyjmuje wektory (100D) i ma 3 warstwy.\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=lstm_hidden_dim, num_layers=num_layers,\n",
    "                              batch_first=True, dropout=dropout, bidirectional=False) # WAŻNE: używamy tylko zwykłego LSTM\n",
    "\n",
    "        # dodajemy warstwę uwagi, która \"ogarnie\" wyjścia z LSTM\n",
    "        self.attention = Attention(lstm_hidden_dim)\n",
    "\n",
    "        # warstwa klasyfikująca: przyjmuje wektor kontekstu (output attention) i przewiduje 2 klasy\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x to są nasze wektory słów (embeddingi)\n",
    "        lstm_outputs, (hidden, cell) = self.lstm(x) # przepuszczamy przez LSTM\n",
    "\n",
    "        # wszystkie stany ukryte lecą do Attention, żeby wygenerować wektor kontekstu\n",
    "        context_vector = self.attention(lstm_outputs)\n",
    "\n",
    "        # wektor kontekstu idzie do klasyfikatora\n",
    "        out = self.fc(context_vector)\n",
    "        return out\n",
    "\n",
    "class TextEmbeddingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    customowy dataset pyTorch, który zamienia ID słów na gotowe wektory Word2Vec.\n",
    "    dzięki niemu model dostaje od razu wektory, a nie ID.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences_padded, labels, embedding_matrix):\n",
    "        self.sequences = sequences_padded\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        # macierz embeddingów przerabiamy na tensor PyTorch\n",
    "        self.embedding_matrix = torch.from_numpy(embedding_matrix)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences) # zwraca ile mamy recenzji\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.sequences[idx] # pobieramy ID słów dla recenzji\n",
    "\n",
    "        # KLUCZOWY KROK: zamieniamy te ID na wektory z naszej macierzy Word2Vec\n",
    "        embeddings = torch.index_select(self.embedding_matrix, 0, torch.LongTensor(indices))\n",
    "\n",
    "        # zwracamy wektory (X) i etykietę (Y)\n",
    "        return embeddings, self.labels[idx]\n",
    "\n",
    "# --- KROK 1: Pobieranie i wczytywanie danych IMDb z Kaggle ---\n",
    "print(\"pobieranie zbioru danych IMDb z Kaggle Hub...\")\n",
    "# używamy kagglehub do pobrania gotowego pliku csv\n",
    "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "csv_file_path = os.path.join(path, \"IMDB Dataset.csv\")\n",
    "# ładujemy dane do pandas\n",
    "df = pd.read_csv(csv_file_path)\n",
    "print(\"pobieranie i wczytywanie zakończone.\")\n",
    "print(f\"wczytano {len(df)} recenzji.\")\n",
    "\n",
    "# upraszczanie etykiet\n",
    "all_reviews_text = df['review'].tolist()\n",
    "# zamieniamy 'positive' i 'negative' na 1 i 0\n",
    "all_labels = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n",
    "\n",
    "# podział na zbiór treningowy i testowy (80/20)\n",
    "X_train_text, X_test_text, Y_train, Y_test = train_test_split(\n",
    "    all_reviews_text, all_labels, test_size=TEST_SIZE, random_state=42, stratify=all_labels)\n",
    "\n",
    "# --- KROK 2: Trenowanie Word2Vec (tworzenie embeddingów) ---\n",
    "# prosta tokenizacja na potrzeby gensim (usuwamy <br /> i interpunkcję, wszystko małe litery)\n",
    "tokenized_corpus = [\n",
    "    re.sub(r'[^a-z\\s]', '', re.sub(r'<br />', ' ', review).lower()).split()\n",
    "    for review in all_reviews_text]\n",
    "\n",
    "print(\"rozpoczynanie trenowania modelu Word2Vec...\")\n",
    "start_w2v_time = time.time()\n",
    "# trenujemy Word2Vec w trybie Skip-gram (sg=1), żeby dostać 100D wektory\n",
    "w2v_model = gensim.models.Word2Vec(\n",
    "    sentences=tokenized_corpus,\n",
    "    vector_size=EMBEDDING_DIM,\n",
    "    window=5,\n",
    "    min_count=W2V_MIN_COUNT,\n",
    "    workers=4,\n",
    "    sg=1 # skip-gram\n",
    ")\n",
    "end_w2v_time = time.time()\n",
    "print(f\"trenowanie Word2Vec zakończone. wymiar embeddingów: {EMBEDDING_DIM}\")\n",
    "print(f\"czas trenowania Word2Vec: {(end_w2v_time - start_w2v_time):.2f} sekund.\")\n",
    "\n",
    "# --- KROK 3: Przygotowanie danych do PyTorch (tokenizacja i padding) ---\n",
    "keras_tokenizer = Tokenizer()\n",
    "# tworzymy słownik ID dla wszystkich słów\n",
    "keras_tokenizer.fit_on_texts(all_reviews_text)\n",
    "word_index = keras_tokenizer.word_index\n",
    "VOCAB_SIZE = len(word_index) + 1 # +1 na padding (ID 0)\n",
    "\n",
    "# tworzymy macierz, która będzie przechowywać wszystkie wektory Word2Vec (VOCAB_SIZE wierszy, EMBEDDING_DIM kolumn)\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM), dtype=np.float32)\n",
    "\n",
    "# wypełnianie macierzy naszymi wytrenowanymi wektorami\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "# zamieniamy teksty na listy ID słów\n",
    "X_train_sequences = keras_tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test_sequences = keras_tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "# PADDING: wyrównujemy długość wszystkich recenzji do 300\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# ustawienia DataLoadera\n",
    "pin_memory = torch.cuda.is_available() # włączenie optymalizacji dla GPU, jeśli jest\n",
    "\n",
    "# tworzenie obiektów DataLoader, które będą podawały dane w batchach do modelu\n",
    "train_dataset = TextEmbeddingDataset(X_train_padded, Y_train, embedding_matrix)\n",
    "test_dataset = TextEmbeddingDataset(X_test_padded, Y_test, embedding_matrix)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=pin_memory)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "# --- KROK 4: Funkcje Treningu i Ewaluacji ---\n",
    "\n",
    "def train_model(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"funkcja do trenowania modelu przez jedną epokę\"\"\"\n",
    "    model.train() # włącza tryb treningowy (włącza dropout)\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache() # czyścimy cache GPU na wszelki wypadek\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        # dane lecą na GPU/CPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # ZEROWANIE GRADIENTÓW (musowe w PyTorch)\n",
    "        outputs = model(inputs) # forward pass (przewidywanie)\n",
    "        loss = criterion(outputs, labels) # obliczenie błędu\n",
    "\n",
    "        loss.backward() # backward pass (obliczenie gradientów)\n",
    "        optimizer.step() # aktualizacja wag\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    \"\"\"funkcja do mierzenia wydajności (bez nauki)\"\"\"\n",
    "    model.eval() # wyłącza tryb treningowy (wyłącza dropout)\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad(): # wyłącza liczenie gradientów (oszczędność czasu/pamięci)\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# --- KROK 5: Inicjalizacja i Pętla Treningowa ---\n",
    "# sprawdzamy czy mamy GPU, inaczej używamy CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"używane urządzenie: {device}\")\n",
    "\n",
    "# inicjalizujemy nasz model i wysyłamy go na wybrane urządzenie\n",
    "model = EmbeddingClassifier(\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    lstm_hidden_dim=LSTM_HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dropout=DROPOUT,\n",
    "   ).to(device)\n",
    "\n",
    "# funkcja straty (błędu)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optymalizator Adam, który aktualizuje wszystkie wagi modelu z naszym learning_rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"\\nrozpoczynanie treningu PyTorch LSTM...\")\n",
    "start_lstm_time = time.time()\n",
    "\n",
    "# pętla treningowa, przechodzimy przez 5 epok\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    # trening na danych treningowych\n",
    "    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
    "    # ewaluacja na danych testowych (jak sobie radzi na nowych danych)\n",
    "    test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "\n",
    "    print(f\"epoka {epoch}/{EPOCHS} (czas trwania: {epoch_duration:.2f} s)\")\n",
    "    print(f\"  trening: loss={train_loss:.4f}, acc={train_acc:.2f}%\")\n",
    "    print(f\"  test:    loss={test_loss:.4f}, acc={test_acc:.2f}%\")\n",
    "\n",
    "end_lstm_time = time.time()\n",
    "total_lstm_time = end_lstm_time - start_lstm_time\n",
    "print(\"trening PyTorch zakończony.\")\n",
    "print(f\"całkowity czas treningu LSTM (dla {EPOCHS} epok): {total_lstm_time:.2f} sekund.\")\n",
    "\n",
    "# --- KROK 6: Zapisanie Wytrenowanego Modelu ---\n",
    "# zapisujemy tylko wagi modelu (state_dict), co jest najlepszą praktyką\n",
    "SAVE_PATH = 'imdb_sentiment_model_weights.pth'\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "print(f\"\\nwagi modelu zostały pomyślnie zapisane w pliku: {SAVE_PATH}\")"
   ],
   "id": "751b7e1b48713845"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
