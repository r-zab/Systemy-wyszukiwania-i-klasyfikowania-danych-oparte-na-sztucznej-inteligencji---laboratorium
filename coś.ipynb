{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- SEKCJA IMPORTÓW ---\n",
    "# Importowanie niezbędnych bibliotek.\n",
    "\n",
    "import os  # Biblioteka do interakcji z systemem operacyjnym, np. do zarządzania ścieżkami plików.\n",
    "import re  # Biblioteka do obsługi wyrażeń regularnych, używana do czyszczenia tekstu.\n",
    "import numpy as np  # Podstawowa biblioteka do obliczeń numerycznych, szczególnie na tablicach (macierzach).\n",
    "import pandas as pd  # Biblioteka do manipulacji i analizy danych, głównie do wczytywania plików CSV.\n",
    "import gensim  # Biblioteka do modelowania tematycznego i przetwarzania języka naturalnego, używana do Word2Vec.\n",
    "import torch  # Główna biblioteka do deep learningu (PyTorch), na której budujemy nasz model.\n",
    "import torch.nn as nn  # Moduł PyTorcha zawierający warstwy sieci neuronowych (np. LSTM, Linear).\n",
    "import torch.optim as optim  # Moduł PyTorcha zawierający algorytmy optymalizacyjne (np. Adam).\n",
    "from torch.utils.data import Dataset, DataLoader  # Narzędzia PyTorcha do tworzenia własnych zbiorów danych i ładowania ich w partiach.\n",
    "from sklearn.model_selection import train_test_split  # Funkcja z scikit-learn do podziału danych na zbiór treningowy i testowy.\n",
    "import time  # Biblioteka do mierzenia czasu wykonania operacji.\n",
    "\n",
    "# Importy z TensorFlow/Keras – używane tylko do tokenizacji i paddingu, bo są bardzo wygodne.\n",
    "from tf_keras.preprocessing.text import Tokenizer  # Narzędzie Keras do zamiany słów na unikalne identyfikatory (liczby).\n",
    "from tf_keras.preprocessing.sequence import pad_sequences  # Narzędzie Keras do wyrównywania długości sekwencji (recenzji).\n",
    "import kagglehub  # Biblioteka do łatwego pobierania zbiorów danych bezpośrednio z platformy Kaggle.\n",
    "\n",
    "# Ukrycie mniej ważnych komunikatów od TensorFlow/oneDNN, aby nie zaśmiecały wyjścia konsoli.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# --- KONFIGURACJA STAŁYCH (HIPERPARAMETRY) ---\n",
    "# Ustawienie kluczowych parametrów, które będą sterować procesem uczenia modelu.\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 300   # Maksymalna długość recenzji w słowach. Dłuższe będą ucinane, krótsze uzupełniane zerami.\n",
    "EMBEDDING_DIM = 100         # Wymiar wektora dla każdego słowa w modelu Word2Vec (każde słowo będzie reprezentowane przez 100 liczb).\n",
    "W2V_MIN_COUNT = 5           # Minimalna częstotliwość występowania słowa w korpusie, aby zostało uwzględnione w modelu Word2Vec.\n",
    "LSTM_HIDDEN_DIM = 128       # Liczba neuronów w warstwie ukrytej sieci LSTM.\n",
    "NUM_CLASSES = 2             # Liczba klas wyjściowych (1 dla sentymentu pozytywnego, 0 dla negatywnego).\n",
    "NUM_LAYERS = 3              # Liczba warstw LSTM ułożonych jedna na drugiej (stacked LSTM).\n",
    "DROPOUT = 0.3               # Prawdopodobieństwo \"wyłączenia\" neuronu podczas treningu, technika regularyzacji zapobiegająca przeuczeniu.\n",
    "LEARNING_RATE = 0.001       # Współczynnik uczenia, określa jak \"mocno\" model koryguje swoje wagi w każdej iteracji.\n",
    "EPOCHS = 5                  # Liczba pełnych przejść przez cały zbiór danych treningowych.\n",
    "BATCH_SIZE = 64             # Liczba recenzji przetwarzanych jednocześnie w jednej iteracji treningowej.\n",
    "TEST_SIZE = 0.2             # Procent danych, który zostanie przeznaczony na zbiór testowy (tutaj 20%).\n",
    "\n",
    "# --- DEFINICJE KLAS PYTORCH (MODEL I OBSŁUGA DANYCH) ---\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementacja mechanizmu uwagi (Attention), który pozwala modelowi skupić się na najważniejszych słowach w recenzji.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        # Inicjalizator klasy, dziedziczy po bazowej klasie `nn.Module` z PyTorcha.\n",
    "        super(Attention, self).__init__()\n",
    "        # Definiujemy jedną warstwę liniową (w pełni połączoną), która nauczy się przypisywać \"wagę\" każdemu słowu.\n",
    "        # Wejście ma rozmiar `hidden_dim`, wyjście to pojedyncza liczba. `bias=False` to drobna optymalizacja.\n",
    "        self.attention_layer = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, lstm_outputs):\n",
    "        # Metoda `forward` definiuje, jak dane przepływają przez warstwę.\n",
    "        # `lstm_outputs` ma kształt: (rozmiar_batcha, długość_sekwencji, wymiar_ukryty).\n",
    "\n",
    "        # Przepuszczamy wyjścia z LSTM przez naszą warstwę liniową, aby uzyskać surowe wagi uwagi dla każdego słowa.\n",
    "        # `squeeze(2)` usuwa ostatni wymiar, który jest zbędny (zmienia kształt z [batch, seq, 1] na [batch, seq]).\n",
    "        attention_weights = self.attention_layer(lstm_outputs).squeeze(2)\n",
    "\n",
    "        # Używamy funkcji softmax, aby znormalizować wagi. Dzięki temu sumują się do 1,\n",
    "        # co można interpretować jako procentowe znaczenie każdego słowa w sekwencji.\n",
    "        soft_attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "\n",
    "        # Mnożymy wagi przez wyjścia LSTM, aby stworzyć \"wektor kontekstu\".\n",
    "        # `bmm` to mnożenie macierzy dla batchy. Wektor ten to ważona suma stanów ukrytych,\n",
    "        # gdzie większy wkład mają słowa z wyższą wagą uwagi.\n",
    "        context_vector = torch.bmm(soft_attention_weights.unsqueeze(1), lstm_outputs).squeeze(1)\n",
    "\n",
    "        # Zwracamy wektor kontekstu, który reprezentuje całą recenzję ze skupieniem na kluczowych słowach.\n",
    "        return context_vector\n",
    "\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Główna architektura modelu: wektory słów (embeddings) -> LSTM -> Attention -> Klasyfikator.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, lstm_hidden_dim, num_layers, num_classes, dropout=0.3):\n",
    "        # Inicjalizator głównej klasy modelu.\n",
    "        super(EmbeddingClassifier, self).__init__()\n",
    "\n",
    "        # Definiujemy warstwę LSTM.\n",
    "        # `input_size` to wymiar wektora wejściowego (nasz EMBEDDING_DIM).\n",
    "        # `hidden_size` to liczba neuronów w warstwie ukrytej.\n",
    "        # `num_layers` to liczba warstw LSTM.\n",
    "        # `batch_first=True` oznacza, że wymiar batcha jest pierwszy w kształcie tensora wejściowego.\n",
    "        # `dropout` dodaje regularyzację między warstwami LSTM.\n",
    "        # `bidirectional=False` oznacza, że używamy jednokierunkowego LSTM (przetwarza sekwencję od początku do końca).\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=lstm_hidden_dim, num_layers=num_layers,\n",
    "                              batch_first=True, dropout=dropout, bidirectional=False)\n",
    "\n",
    "        # Inicjalizujemy naszą własną warstwę Attention, podając jej wymiar warstwy ukrytej LSTM.\n",
    "        self.attention = Attention(lstm_hidden_dim)\n",
    "\n",
    "        # Definiujemy ostatnią warstwę, w pełni połączoną (fully connected), która dokona ostatecznej klasyfikacji.\n",
    "        # Przyjmuje wektor kontekstu z warstwy Attention i zwraca logity dla każdej z `num_classes`.\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Definicja przepływu danych przez cały model.\n",
    "        # `x` to batch danych wejściowych (wektory słów).\n",
    "        # Przepuszczamy dane przez LSTM. Otrzymujemy `lstm_outputs` (stany ukryte dla każdego kroku czasowego)\n",
    "        # oraz `hidden` i `cell` (ostatnie stany ukryte i komórki).\n",
    "        lstm_outputs, (hidden, cell) = self.lstm(x)\n",
    "\n",
    "        # Wyjścia z LSTM przekazujemy do naszej warstwy Attention, aby uzyskać wektor kontekstu.\n",
    "        context_vector = self.attention(lstm_outputs)\n",
    "\n",
    "        # Wektor kontekstu jest przekazywany do warstwy klasyfikującej w celu uzyskania ostatecznych predykcji.\n",
    "        out = self.fc(context_vector)\n",
    "        # Zwracamy wynik klasyfikacji.\n",
    "        return out\n",
    "\n",
    "class TextEmbeddingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Niestandardowy Dataset PyTorcha, który w locie zamienia sekwencje ID słów na gotowe wektory Word2Vec.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences_padded, labels, embedding_matrix):\n",
    "        # Inicjalizator Datasetu.\n",
    "        self.sequences = sequences_padded  # Przechowuje dopełnione sekwencje ID słów.\n",
    "        self.labels = torch.LongTensor(labels)  # Przechowuje etykiety jako tensory PyTorcha typu Long.\n",
    "        # Przechowuje macierz embeddingów (wektorów słów) jako tensor PyTorcha.\n",
    "        self.embedding_matrix = torch.from_numpy(embedding_matrix)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Metoda wymagana przez PyTorch; zwraca całkowitą liczbę próbek w zbiorze danych.\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Metoda wymagana przez PyTorch; pobiera jedną próbkę (recenzję) na podstawie jej indeksu `idx`.\n",
    "        indices = self.sequences[idx]  # Pobieramy sekwencję ID słów dla danej recenzji.\n",
    "\n",
    "        # KLUCZOWY KROK: Używamy `torch.index_select`, aby wybrać wektory z `embedding_matrix`\n",
    "        # odpowiadające indeksom (ID słów) w naszej sekwencji. To zamienia listę liczb na listę wektorów.\n",
    "        embeddings = torch.index_select(self.embedding_matrix, 0, torch.LongTensor(indices))\n",
    "\n",
    "        # Zwracamy parę: tensor z wektorami słów (dane wejściowe `X`) i etykietę (`Y`).\n",
    "        return embeddings, self.labels[idx]\n",
    "\n",
    "# --- KROK 1: POBIERANIE I WCZYTYWANIE DANYCH ---\n",
    "print(\"pobieranie zbioru danych IMDb z Kaggle Hub...\")\n",
    "# Używamy biblioteki kagglehub do pobrania zbioru danych. Zwraca ona ścieżkę do pobranego folderu.\n",
    "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "# Tworzymy pełną ścieżkę do pliku CSV wewnątrz pobranego folderu.\n",
    "csv_file_path = os.path.join(path, \"IMDB Dataset.csv\")\n",
    "# Wczytujemy plik CSV do obiektu DataFrame biblioteki pandas.\n",
    "df = pd.read_csv(csv_file_path)\n",
    "print(\"pobieranie i wczytywanie zakończone.\")\n",
    "print(f\"wczytano {len(df)} recenzji.\")\n",
    "\n",
    "# Wyodrębniamy tekst recenzji do listy.\n",
    "all_reviews_text = df['review'].tolist()\n",
    "# Zamieniamy etykiety tekstowe ('positive', 'negative') na liczbowe (1, 0) za pomocą funkcji lambda.\n",
    "all_labels = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n",
    "\n",
    "# Dzielimy dane na zbiór treningowy i testowy w proporcji 80/20.\n",
    "# `random_state=42` zapewnia powtarzalność podziału.\n",
    "# `stratify=all_labels` zapewnia, że proporcje klas (pozytywnych/negatywnych) będą takie same w obu zbiorach.\n",
    "X_train_text, X_test_text, Y_train, Y_test = train_test_split(\n",
    "    all_reviews_text, all_labels, test_size=TEST_SIZE, random_state=42, stratify=all_labels)\n",
    "\n",
    "# --- KROK 2: TRENOWANIE WORD2VEC (TWORZENIE EMBEDDINGÓW) ---\n",
    "# Prosta tokenizacja tekstu na potrzeby gensim: usuwamy tagi HTML, znaki inne niż litery, zamieniamy na małe litery.\n",
    "tokenized_corpus = [\n",
    "    re.sub(r'[^a-z\\s]', '', re.sub(r'<br />', ' ', review).lower()).split()\n",
    "    for review in all_reviews_text]\n",
    "\n",
    "print(\"rozpoczynanie trenowania modelu Word2Vec...\")\n",
    "start_w2v_time = time.time()  # Zapisujemy czas rozpoczęcia treningu Word2Vec.\n",
    "# Trenujemy model Word2Vec na naszym tokenizowanym korpusie.\n",
    "w2v_model = gensim.models.Word2Vec(\n",
    "    sentences=tokenized_corpus,   # Dane wejściowe.\n",
    "    vector_size=EMBEDDING_DIM,    # Wymiar wektora słowa.\n",
    "    window=5,                     # Maksymalna odległość między bieżącym a przewidywanym słowem w zdaniu.\n",
    "    min_count=W2V_MIN_COUNT,      # Ignoruje słowa o niższej częstotliwości.\n",
    "    workers=4,                    # Liczba wątków procesora do użycia.\n",
    "    sg=1                          # Używamy algorytmu Skip-gram (przewiduje kontekst na podstawie słowa).\n",
    ")\n",
    "end_w2v_time = time.time()  # Zapisujemy czas zakończenia.\n",
    "print(f\"trenowanie Word2Vec zakończone. wymiar embeddingów: {EMBEDDING_DIM}\")\n",
    "print(f\"czas trenowania Word2Vec: {(end_w2v_time - start_w2v_time):.2f} sekund.\")\n",
    "\n",
    "# --- KROK 3: PRZYGOTOWANIE DANYCH DO PYTORCH ---\n",
    "keras_tokenizer = Tokenizer()  # Inicjalizujemy tokenizer z Keras.\n",
    "# Budujemy słownik (indeks słów) na podstawie wszystkich tekstów recenzji.\n",
    "keras_tokenizer.fit_on_texts(all_reviews_text)\n",
    "word_index = keras_tokenizer.word_index  # Pobieramy utworzony słownik mapujący słowa na liczby.\n",
    "VOCAB_SIZE = len(word_index) + 1  # Całkowity rozmiar słownictwa (+1, bo indeks 0 jest zarezerwowany dla paddingu).\n",
    "\n",
    "# Tworzymy pustą macierz numpy, która będzie przechowywać wektory słów.\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM), dtype=np.float32)\n",
    "\n",
    "# Wypełniamy macierz wektorami z naszego wytrenowanego modelu Word2Vec.\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v_model.wv:  # Sprawdzamy, czy słowo istnieje w słowniku Word2Vec.\n",
    "        embedding_matrix[i] = w2v_model.wv[word]  # Jeśli tak, przypisujemy jego wektor do macierzy.\n",
    "\n",
    "# Zamieniamy teksty na sekwencje liczb (ID słów) za pomocą tokenizera.\n",
    "X_train_sequences = keras_tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test_sequences = keras_tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "# Wyrównujemy długość wszystkich sekwencji do MAX_SEQUENCE_LENGTH.\n",
    "# `padding='post'` dodaje zera na końcu. `truncating='post'` ucina sekwencje od końca.\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# Ustawienie optymalizacji dla DataLoader, jeśli dostępne jest GPU.\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "# Tworzymy obiekty naszych niestandardowych Datasetów.\n",
    "train_dataset = TextEmbeddingDataset(X_train_padded, Y_train, embedding_matrix)\n",
    "test_dataset = TextEmbeddingDataset(X_test_padded, Y_test, embedding_matrix)\n",
    "# Tworzymy obiekty DataLoader, które będą dostarczać dane do modelu w partiach (batchach).\n",
    "# `shuffle=True` losowo miesza dane treningowe w każdej epoce.\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=pin_memory)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "# --- KROK 4: FUNKCJE TRENINGU I EWALUACJI ---\n",
    "def train_model(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Funkcja do trenowania modelu przez jedną epokę.\"\"\"\n",
    "    model.train()  # Ustawia model w tryb treningowy (włącza m.in. dropout).\n",
    "    total_loss = 0  # Suma błędów (straty) z całej epoki.\n",
    "    correct = 0  # Liczba poprawnie sklasyfikowanych próbek.\n",
    "    total = 0  # Całkowita liczba próbek.\n",
    "\n",
    "    if device.type == 'cuda':  # Jeśli używamy GPU...\n",
    "        torch.cuda.empty_cache()  # Czyścimy pamięć podręczną GPU.\n",
    "\n",
    "    for inputs, labels in loader:  # Iterujemy po partiach danych z DataLoadera.\n",
    "        # Przenosimy dane (wektory i etykiety) na wybrane urządzenie (CPU lub GPU).\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zerujemy gradienty z poprzedniej iteracji (bardzo ważne w PyTorch).\n",
    "        outputs = model(inputs)  # Przepuszczamy dane przez model, aby uzyskać predykcje (forward pass).\n",
    "        loss = criterion(outputs, labels)  # Obliczamy błąd (stratę) między predykcjami a prawdziwymi etykietami.\n",
    "\n",
    "        loss.backward()  # Obliczamy gradienty straty względem wag modelu (backward pass).\n",
    "        optimizer.step()  # Aktualizujemy wagi modelu na podstawie obliczonych gradientów.\n",
    "\n",
    "        total_loss += loss.item()  # Dodajemy stratę z bieżącej partii do sumy.\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Wybieramy klasę z najwyższym prawdopodobieństwem jako predykcję.\n",
    "        total += labels.size(0)  # Zwiększamy licznik przetworzonych próbek.\n",
    "        correct += (predicted == labels).sum().item()  # Zliczamy poprawne predykcje.\n",
    "\n",
    "    avg_loss = total_loss / len(loader)  # Obliczamy średnią stratę na partię.\n",
    "    accuracy = 100 * correct / total  # Obliczamy celność (accuracy) w procentach.\n",
    "    return avg_loss, accuracy  # Zwracamy średnią stratę i celność.\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    \"\"\"Funkcja do ewaluacji wydajności modelu na zbiorze danych.\"\"\"\n",
    "    model.eval()  # Ustawia model w tryb ewaluacji (wyłącza m.in. dropout).\n",
    "    total_loss = 0  # Suma błędów (straty).\n",
    "    correct = 0  # Liczba poprawnych predykcji.\n",
    "    total = 0  # Całkowita liczba próbek.\n",
    "\n",
    "    with torch.no_grad():  # Wyłączamy obliczanie gradientów, co przyspiesza ewaluację i oszczędza pamięć.\n",
    "        for inputs, labels in loader:  # Iterujemy po partiach danych.\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Przenosimy dane na odpowiednie urządzenie.\n",
    "\n",
    "            outputs = model(inputs)  # Uzyskujemy predykcje z modelu.\n",
    "            loss = criterion(outputs, labels)  # Obliczamy stratę.\n",
    "            total_loss += loss.item()  # Sumujemy stratę.\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Wybieramy przewidywaną klasę.\n",
    "            total += labels.size(0)  # Zliczamy próbki.\n",
    "            correct += (predicted == labels).sum().item()  # Zliczamy poprawne predykcje.\n",
    "\n",
    "    avg_loss = total_loss / len(loader)  # Obliczamy średnią stratę.\n",
    "    accuracy = 100 * correct / total  # Obliczamy celność.\n",
    "    return avg_loss, accuracy  # Zwracamy wyniki.\n",
    "\n",
    "# --- KROK 5: INICJALIZACJA I PĘTLA TRENINGOWA ---\n",
    "# Sprawdzamy, czy dostępne jest GPU (CUDA) i ustawiamy odpowiednie urządzenie.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"używane urządzenie: {device}\")\n",
    "\n",
    "# Tworzymy instancję naszego modelu, przekazując zdefiniowane hiperparametry.\n",
    "model = EmbeddingClassifier(\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    lstm_hidden_dim=LSTM_HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dropout=DROPOUT,\n",
    "   ).to(device)  # Przenosimy cały model na wybrane urządzenie (GPU/CPU).\n",
    "\n",
    "# Definiujemy funkcję straty. CrossEntropyLoss jest standardem dla problemów klasyfikacyjnych.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Definiujemy optymalizator. Adam jest popularnym i skutecznym wyborem.\n",
    "# `model.parameters()` przekazuje wszystkie wagi modelu do optymalizatora.\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"\\nrozpoczynanie treningu PyTorch LSTM...\")\n",
    "start_lstm_time = time.time()  # Zapisujemy czas rozpoczęcia całej pętli treningowej.\n",
    "\n",
    "# Główna pętla treningowa, która iteruje przez zadaną liczbę epok.\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()  # Mierzymy czas startu epoki.\n",
    "\n",
    "    # Uruchamiamy funkcję treningową na zbiorze treningowym.\n",
    "    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
    "    # Uruchamiamy funkcję ewaluacyjną na zbiorze testowym, aby sprawdzić, jak model generalizuje.\n",
    "    test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "    epoch_end_time = time.time()  # Mierzymy czas końca epoki.\n",
    "    epoch_duration = epoch_end_time - epoch_start_time  # Obliczamy czas trwania epoki.\n",
    "\n",
    "    # Wyświetlamy wyniki dla bieżącej epoki.\n",
    "    print(f\"epoka {epoch}/{EPOCHS} (czas trwania: {epoch_duration:.2f} s)\")\n",
    "    print(f\"  trening: loss={train_loss:.4f}, acc={train_acc:.2f}%\")\n",
    "    print(f\"  test:    loss={test_loss:.4f}, acc={test_acc:.2f}%\")\n",
    "\n",
    "end_lstm_time = time.time()  # Zapisujemy czas zakończenia całego treningu.\n",
    "total_lstm_time = end_lstm_time - start_lstm_time  # Obliczamy całkowity czas treningu.\n",
    "print(\"trening PyTorch zakończony.\")\n",
    "print(f\"całkowity czas treningu LSTM (dla {EPOCHS} epok): {total_lstm_time:.2f} sekund.\")\n",
    "\n",
    "# --- KROK 6: ZAPISANIE WYTRENOWANEGO MODELU ---\n",
    "# Definiujemy ścieżkę do pliku, w którym zapiszemy wagi modelu.\n",
    "SAVE_PATH = 'imdb_sentiment_model_weights.pth'\n",
    "# Zapisujemy tylko \"słownik stanu\" (state_dict) modelu, czyli nauczone wagi i biasy.\n",
    "# Jest to zalecana praktyka, ponieważ jest bardziej elastyczna niż zapisywanie całego obiektu modelu.\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "print(f\"\\nwagi modelu zostały pomyślnie zapisane w pliku: {SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
